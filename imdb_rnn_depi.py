# -*- coding: utf-8 -*-
"""IMDB_RNN_DEPI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hKOwHyIzel-WL1prmn9tBwe_evT8-sQS
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.callbacks import Callback
from keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer # tokenizer change the words into numerical values

data=pd.read_csv('/content/drive/MyDrive/IMDB Dataset.csv')
data.head()

data.replace('positive',1,inplace=True)
data.replace('negative',0,inplace=True)

data.head(2)

train_data,test_data=train_test_split(data,test_size=0.2,random_state=42)

train_data.shape,test_data.shape

tokenizer=Tokenizer(num_words=5000)#the model will locate the most 5000 used words inside the reviews to train on them and ignore the rest
tokenizer.fit_on_texts(train_data['review'])
X_train=pad_sequences(tokenizer.texts_to_sequences(train_data['review']), maxlen=200) # the model will take only 200 words from the reviews
X_test=pad_sequences(tokenizer.texts_to_sequences(test_data['review']), maxlen=200)

print (X_test)

Y_train=train_data['sentiment']
Y_test=test_data['sentiment']

print (Y_train)

model=Sequential()
model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))# recurrent dropout is for the feedback loop
model.add(Dense(1,activation='sigmoid'))

model.summary()

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

model.fit(X_train, Y_train, epochs=5,batch_size=64 #update every patch
                   ,verbose=1 #to see what is happening after the number of thr epochs
                   ,validation_data=(X_test,Y_test))

def predict_sentiment(review):
  sequence=tokenizer.texts_to_sequences([review])
  padded_sequence=pad_sequences(sequence,maxlen=200)
  prediction=model.predict(padded_sequence)
  if prediction>=0.5:
    return 'Positive'
  else:
    return 'Negative'

new_review='This movie is so amazing'
prediction=predict_sentiment(new_review)
print ("the model predicted that this review is: ",prediction)